{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spracovanie textu\n",
    "\n",
    "### Róbert Móro, Jakub Ševcech\n",
    "\n",
    "IAU, 8.11.2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spätnú väzbu nám môžete nechať tu: https://tinyurl.com/iau2018-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pýtať sa môžete na http://slido.com#iau2018-w08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predtým, ako sa pustíme do spracovania textu..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Včera boli v USA voľby do Kongresu a na posty guvernérov!\n",
    "\n",
    "<img src=\"img/uncle_sam.jpg\" alt=\"America needs you!\" style=\"margin-left: auto; margin-right: auto; width:400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Volebné predikcie = pohladenie duše dátového vedca\n",
    "\n",
    "https://www.nytimes.com/interactive/2018/11/06/us/elections/results-house-forecast.html\n",
    "\n",
    "<img src=\"img/forecast_house.png\" alt=\"Predpovede do Snemovne reprezentantov\" style=\"margin-left: auto; margin-right: auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Odporúčam pozrieť, ako boli predikcie počítané\n",
    "\n",
    "**NY Times**\n",
    "* https://www.nytimes.com/2018/03/13/upshot/needle-forecast-pennsylvania-special-election.html\n",
    "* https://www.vox.com/2018/11/6/18068700/midterm-elections-results-predictions-needle\n",
    "\n",
    "**FiveThirtyEight (Nate Silver)**\n",
    "* https://fivethirtyeight.com/features/our-final-forecast-in-the-senate-house-and-gubernatorial-races/\n",
    "* https://fivethirtyeight.com/methodology/how-fivethirtyeights-house-and-senate-models-work/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spracovanie textu\n",
    "\n",
    "## Text processing, Natural Language Processing, NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cieľom NLP je naučiť stroj rozumieť ľudskej reči (hovorenej/písanej)\n",
    "\n",
    "<img src=\"img/bender.jpg\" alt=\"Does not compute\" style=\"margin-left: auto; margin-right: auto; width: 600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Schopnosť viesť dialóg s človekom ako test inteligencie - Turingov test (1950)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Čo to znamená, porozumieť textu?\n",
    "\n",
    "1. **Morfologická úroveň** - ako sú tvorené slová\n",
    "2. **Syntaktická úroveň** - ako sa slová spájajú do viet\n",
    "3. **Sémantická úroveň** - význam slov/viet\n",
    "4. **Pragmatická úroveň** - význam v kontexte danej situácie alebo všeobecného poznania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Povedal mu: \"Mier!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mier -> mier (podstatné meno) alebo mieriť (sloveso)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Povedal -> povedať"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Kto komu? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ľudia to strojom (ani iným ľuďom) niekedy vôbec neuľahčujú..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo.](https://en.wikipedia.org/wiki/Buffalo_buffalo_Buffalo_buffalo_buffalo_buffalo_Buffalo_buffalo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Bizón z Buffala, ktorého šikanuje bizón z Buffala, tiež šikanuje bizóna z Buffala.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Si ton tonton tond ton tonton, ton tonton sera tondu par ton tonton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Ak tvoj strýko oholí tvojho strýka, bude tvoj strýko oholený tvojím strýkom.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Čiastkové úlohy NLP\n",
    "\n",
    "* Segmentácia textu\n",
    "  - na tokeny\n",
    "  - na vety (sentence boundary disambiguation)\n",
    "  - na témy\n",
    "* Extrakcia informácií\n",
    "  - nap. rozpoznávanie menných entít (named entity recognition)\n",
    "* Automatická sumarizácia textu\n",
    "* Analýza sentimentu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Čiastkové úlohy NLP\n",
    "\n",
    "* Strojový preklad textu\n",
    "* (Syntaktické) parsovanie viet\n",
    "* Analýza diskurzu, odpovedanie na otázky\n",
    "* Rozpoznávanie a segmentácia reči\n",
    "* Speech-to-text, text-to-speech\n",
    "* Generovanie textu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nás však zaujíma predovšetkým extrakcia čŕt z textu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Aby sme mohli text klasifikovať, hľadať zhluky podobných dokumentov a pod."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Príklad: Chceme rozlíšiť, kto je autorom textu\n",
    "\n",
    "Edgar Allan Poe vs. Mary Shelley vs. HP Lovecraft: https://www.kaggle.com/c/spooky-author-identification\n",
    "\n",
    "**Aké črty (features) by som mohol extrahovať z viet?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goto: Otázka v Slido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Potenciálne užitočné črty\n",
    "\n",
    "* Dĺžka vety\n",
    "* Počet slov vo vete\n",
    "* Priemerná dĺžka slov vo vete\n",
    "* Zložitosť vety (tzv. text readability metrics, napr. Flesh-Kincaid)\n",
    "* Počet spojok/predložiek/iných slovných druhov\n",
    "* **Frekvencia použitých slov - prevod vety (textu) do vektorovej reprezentácie**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vo všeobecnosti\n",
    "\n",
    "Segmentácia textu \n",
    "\n",
    "Prevod textu do vektorovej reprezentácie (tzv. *bag of words*)\n",
    "\n",
    "Identifikácia kľúčových slov, resp. často sa spolu vyskytujúcich slov (tokenov)\n",
    "\n",
    "Určovanie podobnosti dvoch textových dokumentov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nástroje na spracovanie textu v Pythone\n",
    "\n",
    "[NLTK](http://www.nltk.org/)\n",
    "\n",
    "[Gensim](https://radimrehurek.com/gensim/tutorial.html)\n",
    "\n",
    "[sklearn.feature_extraction.text](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "\n",
    "### Nástroje (mimo Pythonu)\n",
    "\n",
    "[Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/) - rozhranie aj cez NLTK\n",
    "\n",
    "[Apache OpenNLP](https://opennlp.apache.org/)\n",
    "\n",
    "[WordNet](https://wordnet.princeton.edu/) - rozhranie cez NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extrakcia čŕt sa robí aj s inými typmi vstupov\n",
    "\n",
    "Obrázky (sklearn.feature_extraction.image, [scikit-image](https://scikit-image.org/))\n",
    "\n",
    "Videá ([scikit-video](http://www.scikit-video.org/stable/))\n",
    "\n",
    "Signál, napr. zvuk ([scikit-signal](https://docs.scipy.org/doc/scipy/reference/signal.html), [scikit-sound](http://work.thaslwanter.at/sksound/html/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Metódy na spracovanie textu\n",
    "\n",
    "Regulárne výrazy, konečné automaty, bezkontextové gramatiky\n",
    "\n",
    "Pravidlové, slovníkové prístupy\n",
    "\n",
    "Prístupy strojového učenia (Markovovské modely, hlboké neurónové siete)\n",
    "\n",
    "Veľmi dobrý prehľad: [Dan Jurafsky, James H. Martin: Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) (aktuálne v príprave 3. vydanie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pozor: Väčšina metód je jazykovo-závislá\n",
    "\n",
    "Veľa dostupných modelov pre angličtinu, nemčinu, španielčinu, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pre slovenčinu je toho výrazne menej\n",
    "\n",
    "Ale nie je to také zúfalé...\n",
    "\n",
    "[Text@FIIT STU](http://text.fiit.stuba.sk/)\n",
    "\n",
    "[NLP4SK](http://arl6.library.sk/nlp4sk/)\n",
    "\n",
    "[Slovenský národný korpus](https://korpus.sk/)\n",
    "\n",
    "[word2vec](https://github.com/essential-data/word2vec-sk)\n",
    "\n",
    "Odporúčaný kontakt na FIIT: doc. Marián Šimko, doc. Peter Lacko, dr. Miroslav Blšták, prípadne doc. Michal Kompan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reprezentácia textu\n",
    "\n",
    "Textový dokument väčšinou reprezentujeme pomocou množiny slov (angl. *bag-of-words*) = vektorom. Zložky vektora predstavujú jednotlivé slová, resp. n-gramy zo slovníka (pre celý korpus/jazyk). Hodnotou zložiek vektora môže byť:\n",
    "\n",
    "* výskyt (binárne)\n",
    "* početnosť\n",
    "* frekvencia\n",
    "* váhovaná frekvencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prevod textu na vektor\n",
    "\n",
    "1. Tokenizácia (rozdelenie textu na vety, následne na slová)\n",
    "2. Normalizácia textu\n",
    "   - prevod na malé písmená\n",
    "   - stemming alebo lematizácia\n",
    "   - odstránenie stopslov (spojky, predložky a pod.)\n",
    "3. Vytvorenie slovníka\n",
    "4. Vytvorenie vektora - zložky slová zo slovníka; väčšinou riedky (angl. *sparse*; veľa núl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenizácia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"\"\"At eight o'clock on Thursday morning \n",
    "... Arthur didn't feel very good. He closed his eyes and went to bed again.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"At eight o'clock on Thursday morning \\n... Arthur didn't feel very good.\", 'He closed his eyes and went to bed again.']\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', '...', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n"
     ]
    }
   ],
   "source": [
    "sent = sentences[0]\n",
    "\n",
    "tokens = nltk.word_tokenize(sent)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Normalizácia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['at', 'eight', \"o'clock\", 'on', 'thursday', 'morning', 'arthur', 'did', \"n't\", 'feel', 'very', 'good']\n"
     ]
    }
   ],
   "source": [
    "tokens = [token.lower() for token in tokens if token not in \".,?!...\"]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stemming alebo lematizácia?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Stemming vráti koreň slova. Napr. *ryba -> ryb*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lematizácia prevádza slová na ich základný slovníkový tvar. Napr. *rybe -> ryba*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Je to vždy jedno alebo druhé.** Prevod na koreň slova sa používa viac pri málo flexných jazykoch (napr. angličtina). Pri flexných jazykoch (napr. slovenčina) je preferovaná lematizácia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Stemming** - pre angličtinu napr. [Porterov algoritmus (1980)](https://www.cs.odu.edu/~jbollen/IR04/readings/readings5.pdf)\n",
    "\n",
    "**Lematizácia** - väčšinou slovníkové metódy (morfologická databáza, resp. tvaroslovník); pre slovenčinu: https://korpus.sk/morphology_database.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['at', 'eight', \"o'clock\", 'on', 'thursday', 'morn', 'arthur', 'did', \"n't\", 'feel', 'veri', 'good']\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "stemmed = [porter.stem(token) for token in tokens]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['at', 'eight', \"o'clock\", 'on', 'thursday', 'morning', 'arthur', 'did', \"n't\", 'feel', 'very', 'good']\n"
     ]
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "lemmatized = [wnl.lemmatize(token) for token in tokens]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Odstránenie stopslov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eight', \"o'clock\", 'thursday', 'morn', 'arthur', \"n't\", 'feel', 'veri', 'good']\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "normalized_tokens = [token for token in stemmed if token not in stopwords]\n",
    "print(normalized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prevod na vektorovú reprezentáciu\n",
    "\n",
    "Pracovať budeme s datasetom [20 newsgroups](http://qwone.com/~jason/20Newsgroups/):\n",
    "\n",
    "*\"The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of my knowledge, it was originally collected by Ken Lang, probably for his Newsweeder: Learning to filter netnews paper, though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "Organization: The City University\n",
      "Lines: 14\n",
      "\n",
      "Does anyone know of a good way (standard PC application/PD utility) to\n",
      "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
      "do the same, converting to HPGL (HP plotter) files.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    return [token.lower() for token in tokens if token.isalpha() and token.lower() not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_docs = [preprocess_text(text) for text in twenty_train.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['michael', 'collier', 'subject', 'converting', 'images', 'hp', 'laserjet', 'iii', 'hampton', 'organization', 'city', 'university', 'lines', 'anyone', 'know', 'good', 'way', 'standard', 'pc', 'utility', 'convert', 'files', 'laserjet', 'iii', 'format', 'would', 'also', 'like', 'converting', 'hpgl', 'hp', 'plotter', 'files', 'please', 'email', 'response', 'correct', 'group', 'thanks', 'advance', 'michael', 'michael', 'collier', 'programmer', 'computer', 'unit', 'email', 'city', 'university', 'tel', 'london', 'fax']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robom\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 3), (2, 1), (8, 1), (20, 2), (22, 1), (23, 1), (26, 2), (33, 1), (39, 2), (59, 2), (60, 1), (61, 1), (62, 2), (64, 1), (65, 2), (78, 1), (86, 1), (99, 2), (103, 1), (110, 2), (128, 1), (135, 1), (155, 1), (158, 1), (160, 1), (162, 1), (187, 1), (200, 1), (205, 1), (208, 2), (213, 1), (220, 2), (224, 2), (236, 2), (239, 1), (253, 1), (256, 1), (258, 1), (261, 1), (270, 1), (273, 4), (277, 1), (278, 3), (290, 2), (306, 1), (308, 1), (310, 1), (314, 2), (317, 1), (318, 1), (319, 4), (328, 1), (329, 2), (335, 1), (337, 3), (338, 5), (340, 1), (361, 1), (371, 1), (379, 2), (410, 4), (431, 1), (433, 1), (443, 1), (458, 1), (479, 5), (509, 1), (511, 1), (513, 3), (534, 1), (569, 2), (632, 1), (635, 1), (666, 1), (670, 1), (703, 1), (718, 1), (720, 1), (750, 1), (766, 2), (771, 1), (776, 3), (777, 1), (787, 1), (823, 1), (824, 1), (825, 1), (826, 1), (827, 1), (828, 1), (829, 1), (830, 1), (831, 1), (832, 1), (833, 1), (834, 2), (835, 1), (836, 1), (837, 1), (838, 1), (839, 2), (840, 3), (841, 1), (842, 1), (843, 1), (844, 1), (845, 1), (846, 5), (847, 4), (848, 1), (849, 1), (850, 1), (851, 1), (852, 1), (853, 1), (854, 1), (855, 1), (856, 1), (857, 1), (858, 1), (859, 2), (860, 1), (861, 2), (862, 1), (863, 1), (864, 3), (865, 1), (866, 2), (867, 1), (868, 1), (869, 1), (870, 1), (871, 1), (872, 1), (873, 1), (874, 4), (875, 1), (876, 1), (877, 1), (878, 2), (879, 1), (880, 1), (881, 1), (882, 1), (883, 1), (884, 1), (885, 1), (886, 1), (887, 1), (888, 1), (889, 2), (890, 1), (891, 1), (892, 1), (893, 1), (894, 1), (895, 1), (896, 1), (897, 2), (898, 1), (899, 3), (900, 1), (901, 1), (902, 1), (903, 1), (904, 1), (905, 1), (906, 1), (907, 2), (908, 1), (909, 1), (910, 1), (911, 1), (912, 1), (913, 2), (914, 1), (915, 1), (916, 1), (917, 1), (918, 1), (919, 1), (920, 3), (921, 1), (922, 1), (923, 1), (924, 2), (925, 4), (926, 1), (927, 1), (928, 1), (929, 1), (930, 1), (931, 1), (932, 1), (933, 1), (934, 1), (935, 1), (936, 1), (937, 1), (938, 1), (939, 1), (940, 1), (941, 1), (942, 1), (943, 1), (944, 1), (945, 1), (946, 1), (947, 1), (948, 1), (949, 1), (950, 2), (951, 1), (952, 1), (953, 1), (954, 1), (955, 2), (956, 2), (957, 1), (958, 1), (959, 1), (960, 2), (961, 1), (962, 1), (963, 1), (964, 1), (965, 1), (966, 1), (967, 2), (968, 1), (969, 1), (970, 1), (971, 1), (972, 1), (973, 2), (974, 1), (975, 1), (976, 1), (977, 1), (978, 1), (979, 1), (980, 1), (981, 1), (982, 1), (983, 1), (984, 1), (985, 1), (986, 1), (987, 1), (988, 1)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "\n",
    "dictionary = corpora.Dictionary(tokenized_docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "print(corpus[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TF-IDF = term frequency * inverse document frequency\n",
    "\n",
    "`TF` – frekvencia slova v aktuálnom dokumente\n",
    "\n",
    "$$ tf(t,d)=\\frac{f_{t,d}}{\\sum_{t' \\in d}{f_{t',d}}} $$\n",
    "\n",
    "`IDF` – záporný logaritmus pravdepodobnosti výskytu slova v dokumentoch korpusu (rovnaká pre všetky dokumenty)\n",
    "\n",
    "$$ idf(t,D) = -\\log{\\frac{|{d \\in D: t \\in d}|}{N}} = \\log{\\frac{N}{|{d \\in D: t \\in d}|}} $$\n",
    "\n",
    "Rôzne varianty (váhovacie schémy): https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.04549138234160459),\n",
       " (2, 0.018081179652764727),\n",
       " (8, 0.03190242457871024),\n",
       " (20, 0.024646895851439338),\n",
       " (22, 0.013140784831683568),\n",
       " (23, 4.5731503853650784e-05),\n",
       " (26, 0.0013149841529262584),\n",
       " (39, 0.03626102811288039),\n",
       " (59, 0.05617330821375869),\n",
       " (60, 0.027680946075067114)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_model = models.TfidfModel(corpus)\n",
    "tfidf_corpus = tfidf_model[corpus]\n",
    "tfidf_corpus[10][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Podobnosť vektorov\n",
    "\n",
    "Podobnosť pomocou Euklidovskej vzdialenosti\n",
    "\n",
    "$$ sim(u,v) = 1- d(u,v) = 1 - \\sqrt{\\sum_{i=1}^{n}{(v_i-u_i)^2}} $$\n",
    "\n",
    "Kosínusová podobnosť\n",
    "\n",
    "$$ sim(u,v) = cos(u,v) = \\frac{u \\cdot v}{||u||||v||} =\\frac{\\sum_{i=1}^{n}{u_iv_i}}{\\sum_{i=1}^{n}{u_i}\\sum_{i=1}^{n}{v_i}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.00336782, 0.01390726, ..., 0.00478842, 0.00904795,\n",
       "       0.00284771], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = similarities.MatrixSimilarity(tfidf_corpus)\n",
    "index[tfidf_corpus[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extrakcia čŕt pomocou scikit-learn\n",
    "\n",
    "http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "\n",
    "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35482)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(stop_words='english')\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4683\n"
     ]
    }
   ],
   "source": [
    "print(count_vect.vocabulary_.get(u'algorithm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Natrénujeme klasifikátor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sprehľadnenie a automatizácia predspracovania: Pipelines\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "\n",
    "http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_ppl = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ppl.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['soc.religion.christian', 'comp.graphics']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[twenty_train.target_names[cat] for cat in text_ppl.predict(docs_new)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vlastný transformátor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class MyTransformer(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Iné časté úlohy (pred)spracovania textu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rozpoznávanie slovných druhov\n",
    "\n",
    "Part-of-Speech Tagging (POS)\n",
    "\n",
    "Slovný druh, číslo, čas, prípadne ďalšie gramatické kategórie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'NN'), ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN'), ('...', ':'), ('Arthur', 'NNP'), ('did', 'VBD'), (\"n't\", 'RB'), ('feel', 'VB'), ('very', 'RB'), ('good', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('NNP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rozpoznávanie menných entít\n",
    "\n",
    "Osoby, organizácie, miesta a pod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "entities = nltk.chunk.ne_chunk(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree('S', [('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'NN'), ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN'), ('...', ':'), Tree('PERSON', [('Arthur', 'NNP')]), ('did', 'VBD'), (\"n't\", 'RB'), ('feel', 'VB'), ('very', 'RB'), ('good', 'JJ'), ('.', '.')])\n"
     ]
    }
   ],
   "source": [
    "print(entities.__repr__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## N-gramy\n",
    "\n",
    "Vo všobecnosti ide o postupnosť $N$ po sebe idúcich položiek. V texte väčšinou na úrovni slov.\n",
    "\n",
    "bigramy\n",
    "\n",
    "trigramy\n",
    "\n",
    "skipgramy - $k$-skip-$n$-gramy\n",
    "\n",
    "https://books.google.com/ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('At', 'eight'), ('eight', \"o'clock\"), (\"o'clock\", 'on'), ('on', 'Thursday'), ('Thursday', 'morning')]\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(sent)\n",
    "bigrams = list(nltk.bigrams(tokens))\n",
    "print(bigrams[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Dá sa nastaviť aj v `CountVectorizer` transformátore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "analyze('Bi-grams are cool!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WordNet\n",
    "\n",
    "* Lexikálna databáza\n",
    "* Organizovaná pomocou tzv. synsetov (množín synoným)\n",
    "  * Podstatné mená, slovesá, prídavné mená, príslovky\n",
    "* Prepojenia medzi synsetmi\n",
    "  * Antonymá, hyperonymá, hyponymá, holonymá, meronymá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'), Synset('cable_car.n.01')]\n"
     ]
    }
   ],
   "source": [
    "print(wn.synsets('car'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "car = wn.synset('car.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'auto', 'automobile', 'machine', 'motorcar']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car.lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a motor vehicle with four wheels; usually propelled by an internal combustion engine'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he needs a car to get to work']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car.examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('ambulance.n.01'), Synset('beach_wagon.n.01'), Synset('bus.n.04'), Synset('cab.n.03'), Synset('compact.n.03')]\n"
     ]
    }
   ],
   "source": [
    "print(car.hyponyms()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('motor_vehicle.n.01')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('accelerator.n.01'), Synset('air_bag.n.01'), Synset('auto_accessory.n.01'), Synset('automobile_engine.n.01'), Synset('automobile_horn.n.01')]\n"
     ]
    }
   ],
   "source": [
    "print(car.part_meronyms()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('white.n.02.white')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('black')[0].lemmas()[0].antonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## word2vec\n",
    "\n",
    "Každé slovo má naučený vektor reálnych čísel, ktoré reprezentujú rôzne jeho vlastnosti a zachytávajú viaceré lingvistické pravidelnosti. Môžeme počítať podobnosť medzi slovami ako podobnosť dvoch vektorov.\n",
    "\n",
    "<img src=\"img/word2vec.png\" alt=\"Trainig word2vec\" style=\"margin-left: auto; margin-right: auto; width: 600px\"/>\n",
    "\n",
    "Zdroj obrázka: https://skymind.ai/wiki/word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "vector('Paris') - vector('France') + vector('Italy') ~= vector('Rome')\n",
    "\n",
    "vector('king') - vector('man') + vector('woman') ~= vector('queen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "https://medium.com/@mishra.thedeepak/word2vec-in-minutes-gensim-nlp-python-6940f4e00980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "sentences = brown.sents()\n",
    "#model = models.Word2Vec(sentences, min_count=1)\n",
    "#model.save('brown_model')\n",
    "model = models.Word2Vec.load('brown_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('father', 0.9837818145751953), ('husband', 0.9667314291000366), ('wife', 0.948153018951416), ('friend', 0.9334405064582825), ('son', 0.9283027648925781), ('nickname', 0.9258568286895752), ('eagle', 0.9163479804992676), ('addiction', 0.9127334356307983), ('voice', 0.9063712358474731), ('patient', 0.9019753932952881)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robom\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(\"mother\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cereal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robom\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Čo si odniesť z dnešnej prednášky \n",
    "\n",
    "* dbať na znovupoužiteľnosť, automatizovateľnosť a modulárnosť predspracovania - slúžia na to tzv. `Pipelines`\n",
    "* pri práci s textom nám (na tomto predmete) ide primárne o **extrakciu čŕt - prevod textu na vektorovú reprezentáciu**\n",
    "* mnohé metódy spracovania textu sú jazykovo-závislé, ale existujú riešenia aj pre slovenčinu\n",
    "\n",
    "### Nabudúce\n",
    "\n",
    "* redukcia dimenzionality - ako veľarozmerný vektor (reprezentujúci napr. text) zobraziť v menejrozmernom priestore\n",
    "* trénovanie, validovanie a testovanie modelov strojového učenia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Zdroje\n",
    "\n",
    "[Dan Jurafsky, James H. Martin: Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) (aktuálne v príprave 3. vydanie)\n",
    "\n",
    "http://www.nltk.org/book/\n",
    "\n",
    "https://radimrehurek.com/gensim/tutorial.html\n",
    "\n",
    "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
